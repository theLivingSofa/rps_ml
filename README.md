# ðŸ§  Rock Paper Scissors â€“ Image Classification Using Machine Learning

This repository contains a Jupyter Notebook implementing an image classification model to identify hand gestures of **Rock**, **Paper**, or **Scissors**. This is a beginner-friendly computer vision project using Python and machine learning libraries.

## ðŸ“„ Notebook
- `Rock Paper Scissor.ipynb`: The main notebook that builds and evaluates the model.

## ðŸ“š Objective
To create a machine learning model that can accurately classify hand gesture images into one of three categories: Rock, Paper, or Scissors. This serves as a great introduction to supervised image classification using classical techniques.

## ðŸ§° Technologies Used

- **Python 3.x**
- **Jupyter Notebook**
- **NumPy**
- **Matplotlib**
- **TensorFlow / Keras**
- **OpenCV / PIL** (if used for image loading or processing)

## ðŸ—‚ï¸ Dataset
> If you used a public dataset (like from Kaggle), please add a link here.

Example:
- Dataset used: [Rock Paper Scissors Dataset â€“ Kaggle](https://www.kaggle.com/datasets/drgfreeman/rockpaperscissors)

It contains labeled images for each of the three classes in `rock`, `paper`, and `scissors` folders.

## ðŸš€ How to Run This Notebook

1. Clone the repo:
```bash
git clone https://github.com/yourusername/rock-paper-scissors-ml.git
cd rock-paper-scissors-ml
