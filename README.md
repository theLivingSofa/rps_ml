# ğŸ§  Rock Paper Scissors â€“ Image Classification Using Machine Learning

This repository contains a Jupyter Notebook implementing an image classification model to identify hand gestures of **Rock**, **Paper**, or **Scissors**. This is a beginner-friendly computer vision project using Python and machine learning libraries.

## ğŸ“„ Notebook
- `Rock Paper Scissor.ipynb`: The main notebook that builds and evaluates the model.

## ğŸ“š Objective
To create a machine learning model that can accurately classify hand gesture images into one of three categories: Rock, Paper, or Scissors. This serves as a great introduction to supervised image classification using classical techniques.

## ğŸ§° Technologies Used

- **Python 3.x**
- **Jupyter Notebook**
- **NumPy**
- **Matplotlib**
- **TensorFlow / Keras**
- **OpenCV / PIL** (if used for image loading or processing)

## ğŸ—‚ï¸ Dataset
- Dataset used: [Rock Paper Scissors Dataset â€“ Kaggle](https://www.kaggle.com/datasets/livingsofa/rock-paper-scissor-image-dataset)

It contains labeled images for each of the three classes in `rock`, `paper`, and `scissors` folders.

## ğŸš€ How to Run This Notebook

1. Clone the repo:
```bash
git clone https://github.com/theLivingSofa/rps_ml.git
cd rps_ml
